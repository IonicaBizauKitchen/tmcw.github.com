---
layout: post
title: k-means
hn:
categories:
- blog
---

[k-means](http://en.wikipedia.org/wiki/K-means_clustering) clustering is a neat way to group points into groups and combine
those groups into fewer points.

I first noticed k-means in [a polymaps example](http://polymaps.org/ex/cluster.html)
that clustered crime data into pretty circles.
It took a while to get the 'aha' moment.

The common algorithm for calculating k-means is a _[heuristic](http://en.wikipedia.org/wiki/Heuristic)_,
which means that it comes up with a viable solution but not necessarily the best one -
since calculating the true, perfect k-means would take a whole lot longer.
Interestingly, the strategy starts with random points, so each time it runs,
the results are different, even if the source data is the same:

![](https://dl.dropbox.com/u/68059/graphics/kmeans.gif)

k-means is just complicated enough to be tricky to visualize, but it's easier
if you take advantage of the fact that the data can be in `d` dimensions.
On maps, we're using 2D k-means; sometimes 3D is used for colors. 1D is less
popular, but is much easier to visualize.

The input to this version of the algorithm will be a simple one-dimensional
set of numbers, and a **k value** - which indicates **how many clusters (means)
should be produced**.

<img src='/graphics/kmeans-input.png' width='640' height='50' />

If this _k_ value is equal to the size of the data itself, then the set
of clusters is the same as the input:

<img src='/graphics/kmeans-same.png' width='640' height='100' />

If this _k_ value is one, then the eventual output will be the global mean:

<img src='/graphics/kmeans-one.png' width='640' height='100' />

Of course, in between is where things get interesting. The way that
k-means is that when you give it a dataset and a k-value, it chooses
k values _at random_ from that dataset, guessing that those might
be cluster centers.

<img src='/graphics/kmeans-choose.png' width='640' height='100' />
